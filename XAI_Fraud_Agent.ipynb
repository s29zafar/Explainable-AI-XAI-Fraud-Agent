{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de9146b",
   "metadata": {},
   "source": [
    "# XAI Fraud Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7846697",
   "metadata": {},
   "source": [
    "## Phase 1: The ML & Explainability Layer\n",
    "\n",
    "### Import Libraries for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d19ec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saimzafar2002-apple.com/venvs/python_3_12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a3958",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76bb803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_transaction(transaction_row, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Helper function to preprocess a single transaction row.\n",
    "    Returns processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame if it's a dict or Series\n",
    "    if not isinstance(transaction_row, pd.DataFrame):\n",
    "        df_input = pd.DataFrame([transaction_row])\n",
    "    else:\n",
    "        df_input = transaction_row.copy()\n",
    "        \n",
    "    # 1. Drop irrelevant columns\n",
    "    cols_to_drop = ['month', 'device_fraud_count', 'fraud_bool']\n",
    "    df_input = df_input.drop(columns=[c for c in cols_to_drop if c in df_input.columns], errors='ignore')\n",
    "    \n",
    "    # 2. Convert types\n",
    "    for col in df_input.columns:\n",
    "        if col not in ['payment_type', 'employment_status', 'housing_status', 'source', 'device_os']:\n",
    "             df_input[col] = pd.to_numeric(df_input[col], errors='coerce')\n",
    "                \n",
    "    # 3. Handle Missing Values\n",
    "    missing_cols = [\n",
    "        \"prev_address_months_count\", \"current_address_months_count\",\n",
    "        \"bank_months_count\", \"session_length_in_minutes\"\n",
    "    ]\n",
    "    for col in missing_cols:\n",
    "        if col in df_input.columns:\n",
    "             df_input[col] = df_input[col].replace(-1, np.nan)\n",
    "    \n",
    "    # 4. Apply OneHotEncoding\n",
    "    if preprocessor:\n",
    "        try:\n",
    "            X_transformed = preprocessor.transform(df_input)\n",
    "            return X_transformed\n",
    "        except Exception as e:\n",
    "            print(f\"Preprocessing error: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        return df_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f2518",
   "metadata": {},
   "source": [
    "### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd13354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(transaction_row, preprocessor, model_params_path='XGBoostModelParameters.json', model_path='XGBoostModel.json'):\n",
    "    \"\"\"\n",
    "    Takes a single transaction row, preprocesses it, and returns the fraud probability.\n",
    "    \"\"\"\n",
    "    # Load parameters\n",
    "    try:\n",
    "        with open(model_params_path, 'r') as file:\n",
    "            loaded_params = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {model_params_path} not found.\")\n",
    "        return None\n",
    "\n",
    "    X_transformed = preprocess_transaction(transaction_row, preprocessor)\n",
    "    if X_transformed is None:\n",
    "        return None\n",
    "    \n",
    "    X_numpy = X_transformed.to_numpy()\n",
    "\n",
    "    # Load Model (Note: This assumes model file exists)\n",
    "    try:\n",
    "        model = xgb.XGBClassifier(**loaded_params)\n",
    "        model.load_model(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Inference\n",
    "    try:\n",
    "        probability = model.predict_proba(X_numpy)[0, 1]\n",
    "        return float(probability)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f239f",
   "metadata": {},
   "source": [
    "### Create SHAP Explanation Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f290ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_explanation(transaction_data, model, preprocessor):\n",
    "    \"\"\"\n",
    "    Generates a SHAP explanation for a single transaction.\n",
    "    Returns a dictionary with fraud probability and top 3 contributing features.\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    X_transformed = preprocess_transaction(transaction_data, preprocessor)\n",
    "    if X_transformed is None:\n",
    "        return {\"error\": \"Preprocessing failed\"}\n",
    "    \n",
    "    # Ensure we use DataFrame for column names in SHAP\n",
    "    feature_names = preprocessor.get_feature_names_out() if hasattr(preprocessor, 'get_feature_names_out') else X_transformed.columns\n",
    "    X_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer(X_df)\n",
    "    \n",
    "    # Get values for the first (and only) row\n",
    "    # shap_values.values shape is (1, n_features)\n",
    "    # Binary classification: some shap versions output values for both classes, some just one.\n",
    "    # For XGBClassifier binary, it usually outputs log-odds for class 1.\n",
    "    \n",
    "    row_values = shap_values.values[0]\n",
    "    # base_value = shap_values.base_values[0] # Not strictly needed for top 3\n",
    "    data_values = X_df.iloc[0]\n",
    "    \n",
    "    # Calculate probability\n",
    "    prob = model.predict_proba(X_df)[0, 1]\n",
    "    \n",
    "    # Identify top 3 features pushing score HIGHER (positive contribution to fraud class)\n",
    "    # We want features that increase the probability of fraud.\n",
    "    \n",
    "    # Create list of (feature_name, shap_value, feature_value)\n",
    "    contributions = []\n",
    "    \n",
    "    # Handle multi-class output shape if SHAP returns (1, n_features, 2)\n",
    "    if len(row_values.shape) > 1:\n",
    "        # Assuming class 1 is index 1\n",
    "        row_values = row_values[:, 1]\n",
    "\n",
    "    for name, val, feat_val in zip(X_df.columns, row_values, data_values):\n",
    "        contributions.append((name, val, feat_val))\n",
    "    \n",
    "    # Sort by SHAP value descending (highest positive impact first)\n",
    "    contributions.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_3 = contributions[:3]\n",
    "    \n",
    "    top_reasons = []\n",
    "    for name, val, feat_val in top_3:\n",
    "        # Clean up feature name (remove 'cat__' etc if present)\n",
    "        clean_name = str(name).replace('cat__', '').replace('remainder__', '')\n",
    "        \n",
    "        # Format based on value type\n",
    "        if isinstance(feat_val, (int, float)):\n",
    "             reason = f\"{clean_name} = {feat_val:.2f}\"\n",
    "        else:\n",
    "             reason = f\"{clean_name} = {feat_val}\"\n",
    "        \n",
    "        top_reasons.append(reason)\n",
    "        \n",
    "    return {\n",
    "        \"score\": float(prob),\n",
    "        \"top_reasons\": top_reasons\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2443c15c",
   "metadata": {},
   "source": [
    "## Phase 2: The Data Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ef7f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fraud_bool  income  name_email_similarity  prev_address_months_count  \\\n",
      "0               0     0.9               0.335133                         -1   \n",
      "1               0     0.7               0.304756                         -1   \n",
      "2               0     0.9               0.341796                        280   \n",
      "3               0     0.4               0.998801                         -1   \n",
      "4               0     0.9               0.712391                         -1   \n",
      "...           ...     ...                    ...                        ...   \n",
      "48417           1     0.8               0.117968                         -1   \n",
      "48418           0     0.8               0.350479                         -1   \n",
      "48419           0     0.5               0.590204                         12   \n",
      "48420           0     0.1               0.068729                         -1   \n",
      "48421           0     0.5               0.734618                         -1   \n",
      "\n",
      "       current_address_months_count  customer_age  days_since_request  \\\n",
      "0                                58            40            0.031185   \n",
      "1                               133            40            0.005954   \n",
      "2                                13            40            0.733279   \n",
      "3                                10            10            0.031299   \n",
      "4                                31            50            0.006845   \n",
      "...                             ...           ...                 ...   \n",
      "48417                            25            30            0.006299   \n",
      "48418                            40            40            0.016839   \n",
      "48419                             0            20            0.048163   \n",
      "48420                           161            50            0.013336   \n",
      "48421                            39            20            0.023433   \n",
      "\n",
      "       intended_balcon_amount payment_type  zip_count_4w  ...  \\\n",
      "0                   -1.057171           AB           863  ...   \n",
      "1                   -1.059787           AC          1261  ...   \n",
      "2                   -0.843728           AB           634  ...   \n",
      "3                   51.404755           AA           633  ...   \n",
      "4                   -0.741995           AB          1043  ...   \n",
      "...                       ...          ...           ...  ...   \n",
      "48417               -1.353427           AC          1027  ...   \n",
      "48418               -0.517650           AC           215  ...   \n",
      "48419               -1.125871           AB           698  ...   \n",
      "48420               -0.672409           AB           935  ...   \n",
      "48421               -1.168311           AC           706  ...   \n",
      "\n",
      "       bank_months_count  has_other_cards  proposed_credit_limit  \\\n",
      "0                      5                0                  200.0   \n",
      "1                     -1                0                  200.0   \n",
      "2                      1                0                 1500.0   \n",
      "3                     26                0                  200.0   \n",
      "4                      1                1                 1000.0   \n",
      "...                  ...              ...                    ...   \n",
      "48417                  6                0                  200.0   \n",
      "48418                 -1                0                  200.0   \n",
      "48419                  1                0                  500.0   \n",
      "48420                 28                0                 1500.0   \n",
      "48421                 -1                0                  200.0   \n",
      "\n",
      "       foreign_request    source session_length_in_minutes  device_os  \\\n",
      "0                    0  INTERNET                  7.029931  macintosh   \n",
      "1                    0  INTERNET                 11.055040      other   \n",
      "2                    0  INTERNET                  7.285919      linux   \n",
      "3                    0  INTERNET                  3.003448      linux   \n",
      "4                    0  INTERNET                  1.646613    windows   \n",
      "...                ...       ...                       ...        ...   \n",
      "48417                0  INTERNET                  7.084575      linux   \n",
      "48418                0  INTERNET                  3.135352      other   \n",
      "48419                0  INTERNET                 25.053900    windows   \n",
      "48420                0  INTERNET                  5.304547    windows   \n",
      "48421                0  INTERNET                  6.563262      linux   \n",
      "\n",
      "       keep_alive_session device_distinct_emails_8w  device_fraud_count  \n",
      "0                       1                         1                   0  \n",
      "1                       1                         1                   0  \n",
      "2                       1                         1                   0  \n",
      "3                       1                         1                   0  \n",
      "4                       1                         1                   0  \n",
      "...                   ...                       ...                 ...  \n",
      "48417                   0                         1                   0  \n",
      "48418                   1                         1                   0  \n",
      "48419                   0                         1                   0  \n",
      "48420                   1                         1                   0  \n",
      "48421                   1                         1                   0  \n",
      "\n",
      "[48422 rows x 31 columns]\n",
      "\n",
      "Data read from SQLite table:\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import kagglehub\n",
    "\n",
    "# Setup SQLLite connection \n",
    "connection = sqlite3.connect(\"Fraud_Agent.db\")\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sgpjesus/bank-account-fraud-dataset-neurips-2022\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)\n",
    "# ensure we point to a .csv file (dataset_download may return a path without extension)\n",
    "csv_path = str(path) + \"/Base.csv\"\n",
    "\n",
    "# read the CSV into a DataFrame and setup the final test data\n",
    "df = pd.read_csv(csv_path)\n",
    "mask = df[\"month\"] == 7\n",
    "full_test_data = df[mask].sample(frac=0.5).reset_index(drop=True).drop('month',axis=1) \n",
    "\n",
    "print(full_test_data)\n",
    "\n",
    "# Setup a Table in SQL\n",
    "table_name = \"transaction_history\"\n",
    "full_test_data.to_sql(table_name, connection, if_exists='replace', index=False)\n",
    "\n",
    "# Verify the data was written by reading it back into a new DataFrame\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "result_df = pd.read_sql_query(query, connection)\n",
    "print(\"\\nData read from SQLite table:\")\n",
    "#print(result_df)\n",
    "\n",
    "# Close the database connection\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74799e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the database is: 13254656 bytes (via PRAGMA)\n"
     ]
    }
   ],
   "source": [
    "def get_db_size_pragma(db_path):\n",
    "    \"\"\"\n",
    "    Gets the size of a SQLite database in bytes using PRAGMA statements.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Get page count\n",
    "    cursor.execute(\"PRAGMA page_count;\")\n",
    "    page_count = cursor.fetchone()[0]\n",
    "\n",
    "    # Get page size\n",
    "    cursor.execute(\"PRAGMA page_size;\")\n",
    "    page_size = cursor.fetchone()[0]\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # Calculate total size in bytes\n",
    "    size_in_bytes = page_count * page_size\n",
    "    return size_in_bytes\n",
    "\n",
    "db_file_path = \"Fraud_Agent.db\"\n",
    "size = get_db_size_pragma(db_file_path)\n",
    "\n",
    "print(f\"The size of the database is: {size} bytes (via PRAGMA)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89175624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.remove(\"Fraud_Agent.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c920822",
   "metadata": {},
   "source": [
    "### The Vector Store. \n",
    "Write a script to read that PDF, split it into chunks using LangChain's RecursiveCharacterTextSplitter, and save it into a local ChromaDB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fa5a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fraud_Detection_Policy.pdf...\n",
      "Splitting text...\n",
      "Created 3 chunks.\n",
      "Initializing embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 415.15it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB client...\n",
      "Adding documents to collection 'reports'...\n",
      "Successfully added 3 chunks to ChromaDB at ./test_chroma\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Fix for SQLite on Mac (common issue with ChromaDB)\n",
    "import sqlite3\n",
    "import sys\n",
    "if sys.platform.startswith('darwin'):\n",
    "    try:\n",
    "        __import__('pysqlite3')\n",
    "        sys.modules['sqlite3'] = sys.modules['pysqlite3']\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "# Configuration\n",
    "CHROMA_PATH = \"./test_chroma\"\n",
    "CHROMA_COLLECTION_NAME = \"reports\"\n",
    "PDF_PATH = \"Fraud_Detection_Policy.pdf\"\n",
    "\n",
    "def ingest_pdf():\n",
    "    # 1. Load PDF\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        print(f\"Error: {PDF_PATH} not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading {PDF_PATH}...\")\n",
    "    loader = PyPDFLoader(PDF_PATH)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 2. Split Text\n",
    "    print(\"Splitting text...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunked_documents = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(chunked_documents)} chunks.\")\n",
    "\n",
    "    # 3. Initialize Embeddings\n",
    "    print(\"Initializing embeddings...\")\n",
    "    embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # 4. Initialize Chroma Client\n",
    "    print(\"Initializing ChromaDB client...\")\n",
    "    chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "\n",
    "    # 5. Add to Chroma\n",
    "    # The error in the notebook was using os.getenv(CHROMA_COLLECTION_NAME), which returned None.\n",
    "    # We use the variable directly here.\n",
    "    print(f\"Adding documents to collection '{CHROMA_COLLECTION_NAME}'...\")\n",
    "    Chroma.from_documents(\n",
    "        documents=chunked_documents,\n",
    "        embedding=embedding_function,\n",
    "        collection_name=CHROMA_COLLECTION_NAME, # Fixed: Use string directly\n",
    "        client=chroma_client,\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully added {len(chunked_documents)} chunks to ChromaDB at {CHROMA_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_pdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02529dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Quant Modeling)",
   "language": "python",
   "name": "quant_modeling_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
